# 카프카 브로커, 클러스터, 주키퍼

<img src="./2%EA%B0%95.assets/image-20230423232624147.png" alt="image-20230423232624147" style="zoom: 25%;" />

브로커의 역할 => 데이터를 안전하게 분산저장 하는 것. 고 가용성의 핵심

![image-20230423232727556](./2%EA%B0%95.assets/image-20230423232727556.png)

-카프카 클러스터를 실행하기 위해서는 주키퍼가 필요함

-주키퍼의 서로 달느 znode에 클러스터를 지정하면 도미

-*root znode에 각 클러스터별 znode를 생성하고* 클러스터 실행시 root가 아닌 하위 znode로 설정

-카프카 3.0 부터는 주키퍼가 없어도 클러스터 동작 가능

ex) 고객팀 클러스터0, 주문팀 클러스터1, 운영팀 클러스터2... 

ex) 여러 클러스터 주키퍼를 하나의 앙상블로 사용하여 동시 운영가능.

ex) 주키퍼에대한 전체적 영향도 줄이기 위해 하나의 클러스터에 대해 주키퍼 각각 다르게 가져갈 수 있지만, 리소스 낭비가 발생할 수 있어서 하나의 앙상블로 여러 카프카 클러스터 다루는 경우가 있음.

브로커의 역할 2-> 컨트롤러, 데이터 삭제

클러스터의 다수 브로커중 한 대가 컨트롤러의 역할을 한다. 컨트롤러는 다른 브로커들의 상태를 체크 + 브로커가 클러스터에서 빠지는 경우 해당 브로커에 ㅈ노재하는 리더 파티션을 재분배한다.

카프카는 지속적으로 데이터를 처리해야하기 때문에, 브로커가 비정상이라면 빠르게 클러스터에서 빼는 것이 중요하다. 만약 컨트롤러 역할을 하는 브로커에 장애가 생기면 다른 브로커가 컨트롤러 역할을 한다.

데이터의 삭제

카프카는 오직 브로커만 데이터를 삭제할 수 있다.(컨슈머나 프로듀서가 데이터 삭제를 요청할 수도 없다.) 데이터 삭제는 파일 단위로 이루어지는데 이 단위를 '로그 세그먼트' 라고 한다.(시간, 용량에 따라 혹은 compact라는 옵션으로 가장 최근의 메시지 키가 있는 레코드 제외하고 나머지 레코드 키가 있는 것들 삭제 가능) 이 세그먼트에는 다수의 데이터가 들어 있기 때문에 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없다. (어떤 레코드만 핀셋으로 콕 찝어서 삭제 불가.)

브로커의 역할 3-> 컨슈머 오프셋 저장, 코디네이터

commit => 토픽에 있는 데이터를 컨슈머가 가져왔을때 컨슈머가 어느 오프셋까지 처리했는지 알리는것을 커밋이라고 한다. 이를 특정 토픽에서 어느 오프셋까지 처리했는지 저장한다. (인터널 토픽이라고도 한다. 자동으로 해주기때문에)

컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋한다. 커밋한 오프셋은 __consumer_offsets 토픽에 저장한다. 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리한다.

그룹 코디네이터 => 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다. 컨슈머가 컨슈머 그룹에서 빠지면 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 끊임없이 데이터가 처리되도록 도와준다. 이렇게 파티션을 컨슈머로 재할당하는 과정을 리밸런스(rebalance)라고 부른다.

브로커의 역할 핵심1 - 데이터의 저장

카프카를 실행할 때 config/server.properties의 lgo.dir 옵션에 정의한 디렉토리에 데이터를 저장한다.

토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장한다.

-hello.kafka 토픽의 0번 파티션에 존재하는 데이터를 확인할 수 있다. log에는 메시지와 메타데이터를 저장한다. index는 메시지의 오프셋을 인덱싱한 정보를 담은 파일이다. timeindex파일에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담겨있다.

![image-20230423234602484](./2%EA%B0%95.assets/image-20230423234602484.png)

## 로그와 세그먼트

![image-20230423234942417](./2%EA%B0%95.assets/image-20230423234942417.png)

-log.segment.bytes : 바이트 단위의 최대 세그먼트 크기 지정, 기본 값은 1GB.

-log.roll.ms(hours): 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기. 기본 값은 7일

 가장 마지막 세그먼트 파일(쓰기가 일어나고 있는 파일)을 액티브 세그먼트라 부른다. 액티브 세그먼트는 브로커의 삭제 대상으로 포함되지 않는다. 액티브 세그먼트가 아닌 세그먼트는  retention 옵션에 따라 삭제 대상으로 지정된다.

#### 세그먼트의 삭제 주기

 cleanup.policy=delete

-retentions.ms(minutes,hours) : 세그먼트를 보유할 최대 기간 (default = 7days, 근데 보통 근무 토요일,일요일지나고 월요일 3일함)

-retention.bytes : 파티션당 로그 적재 바이트 값. 기본 값은 -1(지정하지 않음)      ex)적재 바이트 1기가면 완료된 세그먼트 기준으로 데이터 삭제.

-log.retention.check.interval.ms:세그먼트가 삭제 영역에 들어왔는지 확인하는 간격. 기본 값은 5분

데이터 너무 많이 들어오는데 retention너무 많이하면 파일 시스템 디스크 크기가 용량 허용할지 잘 판단(ex 하루 1TB인데 7일이면 7TB 근데 디스크 용량은 5TB... 문제 생김)

☆카프카에서는 데이터가 세그먼트 단위로 삭제가 발생하기 때문에 로그 단위로 개별 삭제는 불가능.

또한 로그(레코드)의 메시지 키, 메시지 값, 오프셋, 헤더 등 이미 적재된 데이터에 대해서 수정 또한 불가능하기 때문에 데이터를 적재할 때(프로듀서) 또는 데이터를 사용할 때(컨슈머) 데이터를 검증하는 것이 좋다.

☆프로듀서에서 validate 꼭 해주기. consumer에서도 validation 해주기.

cleanup.policy = compact

![image-20230424000809753](./2%EA%B0%95.assets/image-20230424000809753.png)

메시지에는 offset, key, value로 이루어진다. 토픽 압축 정책은 일반적으로 생각하는 zip과 같은 압축(compression)과는 다른 개념이다. 여기서 압축이란 메시지 키 별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책을 뜻한다. 그렇기 때문에 삭제(delete) 정책과 다르게 일부 레코드만 삭제가 될 수 있다. 압축은 액티브 세그먼트를 제외한 데이터가 대상이다.

## 테일/헤드 영역, 클린/더티 로그

![image-20230424001158597](./2%EA%B0%95.assets/image-20230424001158597.png)

-테일 영역: 압축 정책에 의해 압축이 완료된 레코드들. 클린(clean)로그 라고도 부른다. 중복 메시지 키가 없다.

-헤드 영역: 압축 정책이 되기 전 레코드들. 더티(dirty)로그 라고도 부른다. 중복된 메시지 키가 있다.

![image-20230424001303859](./2%EA%B0%95.assets/image-20230424001303859.png)

min.cleanable.dirty.ratio (데이터의 압축 시작 시점 옵션값)은 액티브 세그먼트를 제외한 세그먼트에 남아 있는 테일 영역의 레코드 개수와 헤드 영역의 레코드 개수의 비율을 뜻한다. 예를 들어 0.5로 설정한다면 테일 영역의 레코드 개수가 헤드 영역의 레코드 개수와 동일할 경우 압축이 실행된다. 0.9와 같이 크게 설정하면 한번 압축할 때 많은 데이터가 줄어들기 때문에 압축 효과가 좋다. 0.1로 작세 설정하면 압축이 자주 일어나서 가장 최신 데이터만 유지할 수 있지만, 압축이 자주 발생해서 브로커에 부담을 줄 수 있다.

## 브로커의 역할 - 복제(Replication)

![image-20230424001739176](./2%EA%B0%95.assets/image-20230424001739176.png)

데이터 복제(replication)는 카프카를 장애 허용 시스템(fault tolerant system)으로 동작하도록 하는 원동력이다. 복제의 이유는 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터를 유실하지 않고 안전하게 사용하기 위함이다. 카프카의 데이터 복제는 파티션 단위로 이루어진다. 토픽을 생성할 때 파티션의 복제 개수(replication factor)도 같이 설정되는데 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션 값을 따라간다. 복제 개수의 최소값은 1(복제 업음)이고 최대값은 브로커 개수만큼 설정하여 사용할 수 있다.

프로듀서 또는 컨슈머와 직접 통신파는 파티션 => 리더, 나머지 복제 데이터 가진 파티션 => 팔로워

파티션 복제 -> 나머지 브로커에도 파티션의 데이터가 복제되므로 복제 개수만큼의 저장 용량이 증가한다는 단점이 있다. 하지만 안전하게 사용할 수 있다는 강려한 장점때문에 카프카를 운영할 때 2 이상의 복제 개수를 정하는 것이 중요하다. 카프카 브로커가 설치된 기업용 서버는 개인용 컴퓨터와 비교가 안될정도로 안 정성이 좋지만 서버는 해커로 인한 침입, 디스크 오류, 네트워크 연결 장애등의 이유로 언제든 장애가 발생할 수 있다.

![image-20230424002610470](./2%EA%B0%95.assets/image-20230424002610470.png)

브로커 장애 발생경우

=> 브로커가 다운되면 해당 브로커에 있는 리더 파티션은 사용X => 팔로워 파티션 중 하나가 리더 파티션 지위를 넘겨받는다. 이를 통해 데이터가 유지되지 않고 컨슈머나 프로듀서와 데이터를 주고받도록 동작할 수 있다. 운영 시에는 데이터 종류마다 다른 복제 개수를 설정하고 상황에 따라서는 토픽마다 복제 개수를 다르게 설정하여 운영하기도한다. 데이터가 일부 유실되어도 무관하고 데이터 처리 속도가 중요하다면 1 또는 2로 설정한다. 금융 정보와 같이 유실이 일어나면 안되는 데이터의 경우 복제개수(reflication factor)를 3으로 설정하기도 한다.

언제 reflication factor를 1로 할까? ex) GPS => 1분 60개 중간 중간 정보 빠져도 가능 metric같은경우는 1개로 설정 일반적은 2, 금융 정보는 3

## ISR(In-Sync_replicas)

![image-20230424003145249](./2%EA%B0%95.assets/image-20230424003145249.png)

ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻한다.(오프셋의 크기가 같음) 복제 개수가 2인 토픽을 가정해 보자. 이토픽에는 리더 파티션 1개와 팔로워 파티션이 1개가 존재할 것이다. 리더 파티션에는 0부터 3의 오프셋이 있다고 가정할 때, 팔로워 파티션에 동기화가 완료되려면 0부터 3까지 오프셋이 존재해야 한다. 동기화가 오나료됐다는 의미는 리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태를 말하기 때문이다.

![image-20230424003821401](./2%EA%B0%95.assets/image-20230424003821401.png)

리더 파티션의 데이터를 모두 복제하지 못한 상태이고, 이렇게 싱크가 되지 않은 팔로워 파티션이 리더 파티션으로 선출되면 데이터가 유실될 수 있다. 유실이 발생하더라도 서비스를 중단하지 않고 지속적으로 토픽을 사용하고 싶다면 ISR이 아닌 팔로워 파티션을 리더로 선출하도록 설정할 수 있다.

- unclean.leader.election.enable=true : 유실을 감수함. 복제가 안된 팔로워 파티션을 리더로 승급.
- unclean.leader.election.enable=false : 유실을 감수하지 않음. 해당 브로커가 복구될 때까지 중단.(리더 재시작까지 기다림)

## 토픽과 파티션

![image-20230424003848873](./2%EA%B0%95.assets/image-20230424003848873.png)

 

토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위이다. 토픽은 1개 이상의 파티션을 소유하고 있 다. 파티션에는 프로듀서가 보낸 데이터들이 들어가 저장되는데 이 데이터를 '레코드(record)'라고 부른다. 파티션은 자료구조에서 접하는 큐(queue)와 비슷한 구조라고 생각하면 쉽다. First-in-first-out(FIFO) 구 조와 같이 먼저 들어간 레코드는 컨슈머가 먼저 가져가게 된다. 다만, 일반적인 자료구조로 사용되는 큐는 데이터를 가져가면(pop) 삭제하지만 카프카에서는 삭제 하지 않는다. 파티션의 레코드는 컨슈머가 가져가 는 것과 별개로 관리된다. 이러한 특징 때문에 토픽의 레코드는 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러번 가져갈수있다.

![image-20230424004217623](./2%EA%B0%95.assets/image-20230424004217623.png)

#### 토픽 생성시 파티션이 배치되는 방법

파티션이 5개인 토픽을 생성했을 경우 그림과 같이 0번 브로커부터 시작하여 round-robin 방식으로 리더 파티션들이 생성된다. 카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고 받으므 로 여러 브로커에 골고루 네트워크 통신을 하게 된다. 이를 통해, 데이터가 특정 서버(여기서는 브로커)와 통신이 집중되는(hot spot) 현상을 막고 선형 확장(linear scale out)을 하여 데이터가 많아지더라도 자연 스럽게 대응할 수 있게 된다.

![image-20230424005434232](./2%EA%B0%95.assets/image-20230424005434232.png)

### 특정 브로커에 파티션이 쏠린 현상

한쪽 브로커에 CPU, RAM차지가 매우 많아짐. 브로커1, 2가 복제해가서 어느정도 쏠림 막아주겠지만, 쏠리면 안됌.

![image-20230424005558076](./2%EA%B0%95.assets/image-20230424005558076.png)

특정 브로커에 파티션이 몰리는 경우에는 kafka-reassign-partitions.sh shell스크립트 명령으로 파티션 분배 가능.

### 파티션 개수와 컨슈머 개수의 처리량

![image-20230424005758727](./2%EA%B0%95.assets/image-20230424005758727.png)

파티션은 카프카의 병렬처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리 할 수 있도록 매칭된다. 컨슈머의 처리량이 한정된 상황에서 많은 레코드를 병렬로 처리하는 가장 좋은 방법은 컨슈머의 개수를 늘려 스케일 아웃하는 것이다. 컨슈머 개수를 늘림과 동시에 파티션 개수도 늘리면 처리량이 증가 하는 효과를 볼 수있다.

물론 컨슈머가 죽어도, 컨슈머 1개가 나머지 파티션도 할당받아 처리하게 됌. 보통 프로듀서로 초당 10개 들어온다하면 보수적으로 파티션, 컨슈머 초당 20개 정도 처리하게 맞춰준다.

![image-20230424010034016](./2%EA%B0%95.assets/image-20230424010034016.png)

카프카에서 파티션 개수를 줄이는 것은 지원하지 않는다. 그러므로 파티션을 늘리는 작업을 할 때는 신중 히 파티션 개수를 정해야 한다. 한번 늘리면 줄이는 것은 불가능하기 때문에 토픽을 삭제하고 재생성하는 방법 외에는 없기 때문이다. 카프카에서는 파티션의 데이터를 세그먼트로 저장하고 있으며 만에 하나 지원 을 한다고 하더라도 여러 브로커에 저장된 데이터를 취합하고 정렬해야하는 복잡한 과정을 거쳐야 하기 때 문에 클러스터에 큰 영향이 가게 된다. KIP-694에서 파티션 개수를 줄이는 것을 논의했지만 더 이상 진행 되지 않고 있다.

(파티션 합치려면 타임 스탬프 같은거 합쳐주고 인덱스해주고 해야해서 힘듬)