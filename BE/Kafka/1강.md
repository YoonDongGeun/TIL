## 카프카를 쓰는 4가지 이유

### 1. 높은 처리량

프로듀서가 브로커로 데이터를 보낼때, 컨슈머가 브로커로부터 데이터를 받을 때, 모두 묶어서 전송 한다. => 네트워크 비용이 절약된다. 이를 배치 처리한다고 한다. 배치로 빠르게 처리하는 것은 대용량의 실시간 로그 데이터를 처리하는데 적합하다. 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터 병렬처리도 가능하다. (파티션 개수만큼 컨슈머 늘려서 동일 시간당 데이터 처리량 늘리기)

=> 스케일 업 vs 스케일 아웃 => 스케일 아웃은 개수 늘리기, 스케일 업은 성능 높이

### 2. 확장성

가변적인 상황에서 카프카 클러스트의 브로커 개수를 조절하여 안정적인 확장을 할 수 있다. 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃할 수 있고, 데이터가 적어지면 스케일 인 할 수 있다. 카프카의 스케일 인,아웃은 클러스터의 무중단 운영을 지원하기 때문에, 24시간 데이터를 처리해야하는 커머스나 은행 같은 비지니스 모델에서 안정적인 운영이 가능.

### 3. 영속성

영속성이란? 데이터를 생성한 프로그램이 종료되더라도 사라지지 않는 데이터의 특성을 뜻한다.

카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다. 파일 시스템에 데이터를 적재하고 사용하는 것은 보편적으론 느리다고 생각하겠지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용하였다.

운영체제에서는 파일 I/O성능 향상을 위해 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용한다. 페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이기 때문에 카프카가 파일 시스템에 저장하고, 데이터를 저장, 전송 하더라도 처리량이 높은 것이다. 디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 종료되도 다시 프로세스 시작하여 안전하게 데이터를 처리할 수 있다.

### 4. 고가용성

3개 이상의 서버들로 운영되는 카프카 클러스트는 일부 서버에 장애가 발생해도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있습니다.  브로커 여러개(서버 여러개)로 이루어진 클러스터. 프로듀서가 메인 브로커에 보내면 2번 3번 브로커도 복사가 되기 때문에(레플리카) 메인 브로커가 죽어도 서브 브로커 에서 복사한것들로 컨슈머가 소모한다.





## 빅데이터 아키텍처의 종류와 미래

![image-20230423205049785](./1%EA%B0%95.assets/image-20230423205049785.png)

초기 빅데이터 플랫폼은 위와 같이 엔드 투 엔드로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았다.. 원천 데이터는 source application에게 받은 최초의 데이터. 파생데이터는 원천 데이터를 특정 프로세싱을 통해 가공하 한 것이다. 그래서 이 파생데이터를 바탕으로 클라이언트나 서버에서 사용할 수 있도록 만든 것이 서빙 데이터 이다. 그런데 데이터를 배치로 모으는 구조는 유연하지 못했고, 실시간으로 생성된느 데이터들에 대한 인사이트를 서비스 애플리케이션에 빠르게 전달하지 못하는 단점이 있었다. 또한 우너천 데이터로부터 파생된 데이터의 히스토리를 파악하기 어려웠고, 계속되는 데이터 가공으로 데이터 파편화가 발생하면서 데이터 거버넌스(데이터 표준 및 정책)을 지키기 어려웠다.

![image-20230423205626693](./1%EA%B0%95.assets/image-20230423205626693.png)

그래서 나온 데이터 레이크 아키텍처는 람다 아키텍처이다. 배치레이어는 배치 데이터를 모아서 특정 시간마다 데이터를 일괄 처리한다. 스피드 레이어는 서비스에서 생성되는 원천 데이터를 실시간으로 빠르게 처리하는 레이어이다. 카프카가 여기에 해당한다. 그리고 배치와 스피드레이어에서 만든 데이터를 데이터 사이언티스트나 사용자, 클라이언트가 가져갈 수 있도록 만드는 것이 서빙레이어 이다.

하지만 한계가 또한 있었다.

=> 배치 레이어와 (하둡), 스피드레이어(실시간 데이터, 카프카)를 나눠서 만들어야 한다. 그래서 유연하지 못한 파이프라인을 생성해야한다.  1개의 로직을 추상화하여 서밍버드라고 배치 레이어와 스피드레이어에 적용하는 형태의 것이 있지만, 디버깅도 따로, 배포도 따로해야하기 때문에 완전히 해결된 것은 아니다.

![image-20230423224522671](./1%EA%B0%95.assets/image-20230423224522671.png)

파카 아키텍처라는 것이 람다 아키텍처의 단점을 해소하기 위해 나왔다. (디버깅 파편화, 배포의 파편화, 운영의 분리, 해결)

모든 배치데이터를 스피드 레이어에서 해결할 수 있도록 하는것이 핵심. 

=> 카프카가 배치데이터와 스트림 데이터를 모두 모으는 형태

 

![image-20230423225148718](./1%EA%B0%95.assets/image-20230423225148718.png)

로그는 배치 데이터를 스트림으로 표현하기에 적합하다. 위를 보면, 배치 데이터 스냅샷과 변환 기록 로그가 있는데, 일반적으로 데이터 플램폼에서 배치 데이터를 표현할 때는 배치 데이터 스냅샷으로 배치 데이터를 각 시점(시간, 혹은 일자별 등)마다 백업했다. 하지만  변환 기록 로그(change log)로 표현할 때 각 시점의 배치 데이터의 변환 기록을 시간 순서대로 기록함으로써, 각 시점의 몯느 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다.

(카프카에서 레코드당 타임스탬프가 기록된다. 이 데이터를 기반으로 특정 일시부터 특정 일시까지 데이터를 하나로 묶어서 한번에 배치로 메테리얼라이즈 뷰로 가져와서 처리한다.)

![image-20230423225623550](./1%EA%B0%95.assets/image-20230423225623550.png)

![image-20230423225757790](./1%EA%B0%95.assets/image-20230423225757790.png)

![image-20230423225906114](./1%EA%B0%95.assets/image-20230423225906114.png)

각 데이터 노드에 블록단위로 데이터 저장 => 각 데이터 노드에서 맵 리듀스 방식으로 혹은 스파크에서 메모리로 올리는 방법으로 처리한후, HDFS에 데이터 저장하는 방식으로 활용

배치데이터를 처리하는 대표적 방법(하둡에서) => 맵 리듀스 방식 사용 

![image-20230423230045833](./1%EA%B0%95.assets/image-20230423230045833.png)

위는 스트림 데이터를 배치로 사용하는 방법(카프카에서)

Materialized View(구체화된 뷰) 특정일시로 가져와서 뷰로 가져와 한번에 배치처럼 사용 => 키 밸류 스토어 혹은 큰 용량 데이터 분산해서 처리 가능(레코드에 시간을 남기기 때문에 처리 가능)



![image-20230423230239110](./1%EA%B0%95.assets/image-20230423230239110.png)

![image-20230423230537695](./1%EA%B0%95.assets/image-20230423230537695.png)

이 아키텍처는 스트리밍 데이터 레이크 아키텍처이다. 카파 아키텍처에서 서빙 레이어를 제거한 아키텍처이다.

나온 이유는 굳이 분석과 프로세싱이 완료한 대용량 데이터를 오래 저장하고 사용할 수 있다면, 서빙 레이어는 제거되어도 될 것이다라는 것으로 나옴. 서빙 레이어와 스피드 레이어가 이중으로 관리되는 운영 리소스를 줄일 수 있다. 

카파 레이어에서 스트리밍 데이터 레이크로 사용하기 위해서 개선할 부분은 다음과 같다.

자주 접근하지 않는 데이터를 굳이 비싼 자원(브로커의 메모리,디스크)에 유지할 필요가 없다. => 클러스터에서 자주 접근하지 않는 데이터는 오브젝트 스토리지와 같이 저렴하면서도 안전한 저장소에 옮겨 저장하고 자주 사용하는 데이터만 브로커에서 사용하는 구분 작업이 필요하다.

![image-20230423231615811](./1%EA%B0%95.assets/image-20230423231615811.png)

위 커넥트, 프로듀서 스트림즈, 컨슈머는 모두 자바의 카프카 공신 open api 라이브러리.

커넥트와 프로듀서, 컨슈머 다른점은. 클러스터로 따로 운영한다는점.(파이프라인을 템플릿 형태로 반복적으로 여러번 생성할 수 있다.)

MM2(미러 메이커 2) => 클러스터 단위로 카프카를 운영할 때 토픽에 있는 데이터를 완벽하게 복제하기 위해 있는것.